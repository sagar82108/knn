{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d70f60-1874-4c54-ae25-ea708328632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. KNN, or k-Nearest Neighbors, is a supervised machine learning algorithm used for classification and regression tasks. It makes predictions based on the majority class or the average of the k-nearest data points to a given input point in a feature space.\n",
    "\n",
    "Q2. The value of K in KNN is chosen through a process called hyperparameter tuning. It involves experimenting with different values of K and selecting the one that results in the best performance on a validation dataset, typically using techniques like cross-validation.\n",
    "\n",
    "Q3. The main difference between KNN classifier and KNN regressor is in their tasks:\n",
    "   - KNN Classifier: It is used for classification tasks where the goal is to assign a class label to a data point based on the majority class among its k-nearest neighbors.\n",
    "   - KNN Regressor: It is used for regression tasks where the goal is to predict a continuous numerical value based on the average of the values among its k-nearest neighbors.\n",
    "\n",
    "Q4. The performance of KNN can be measured using various metrics depending on the task (classification or regression). Common metrics for classification include accuracy, precision, recall, F1-score, and ROC AUC. For regression, metrics like mean squared error (MSE) and R-squared are often used.\n",
    "\n",
    "Q5. The curse of dimensionality in KNN refers to the problem that as the number of dimensions (features) in the data increases, the distance between data points becomes less meaningful. This can result in increased computational complexity and reduced effectiveness of KNN. Dimensionality reduction techniques or feature selection can help mitigate this issue.\n",
    "\n",
    "Q6. Handling missing values in KNN can be challenging. One approach is to impute missing values by taking the average (for regression) or the mode (for classification) of the available neighbors' values. Alternatively, you can use techniques like k-Nearest Neighbors imputation, but it may introduce bias.\n",
    "\n",
    "Q7. The performance of KNN classifier and regressor depends on the nature of the problem:\n",
    "   - KNN Classifier: It works well for classification tasks when the decision boundaries are not overly complex, and the classes are well-separated. It may not perform well in high-dimensional spaces.\n",
    "   - KNN Regressor: It can be effective for regression tasks when there is a smooth relationship between input features and the target variable. It can handle non-linear relationships.\n",
    "\n",
    "Q8. Strengths of KNN:\n",
    "   - Simple to implement.\n",
    "   - No assumptions about data distribution.\n",
    "   - Can handle non-linear relationships.\n",
    "   \n",
    "   Weaknesses of KNN:\n",
    "   - Sensitive to the choice of K.\n",
    "   - Computationally expensive for large datasets.\n",
    "   - Sensitive to irrelevant features.\n",
    "   - Performs poorly in high-dimensional spaces (curse of dimensionality).\n",
    "   \n",
    "   Addressing weaknesses:\n",
    "   - Use feature selection or dimensionality reduction.\n",
    "   - Carefully choose an appropriate K.\n",
    "   - Implement distance-weighted voting.\n",
    "   - Use efficient data structures like KD-trees for large datasets.\n",
    "\n",
    "Q9. The difference between Euclidean distance and Manhattan distance in KNN lies in how they measure distance between data points:\n",
    "   - Euclidean distance is the straight-line (shortest) distance between two points in a Euclidean space and is computed as the square root of the sum of squared differences between corresponding coordinates.\n",
    "   - Manhattan distance, also known as L1 distance or taxicab distance, is the sum of the absolute differences between corresponding coordinates. It measures distance by moving along grid lines (like navigating a city block).\n",
    "\n",
    "Q10. Feature scaling in KNN is essential because the algorithm relies on the distance between data points. When features have different scales, features with larger scales can dominate the distance calculations. Standardization (scaling features to have mean=0 and standard deviation=1) or normalization (scaling features to a specific range, e.g., [0, 1]) helps ensure that all features contribute equally to distance calculations, improving KNN's performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
